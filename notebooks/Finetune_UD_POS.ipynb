{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iMsxmwsrNfW"
   },
   "source": [
    "```\n",
    "// Copyright 2020 Twitter, Inc.\n",
    "// SPDX-License-Identifier: Apache-2.0\n",
    "```\n",
    "\n",
    "# Finetune Part of Speech Tagging Models\n",
    "\n",
    "Take an existing BERT model (with or without TPP pre-training) and fine-tune it on an Part of Speech Tagging dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Fjip4YztNIh"
   },
   "source": [
    "## Setup libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79yDGUm8s66W"
   },
   "outputs": [],
   "source": [
    "%pip install transformers==3.5.1 datasets==1.1.2 torch==1.4.0 seqeval==1.2.2 gensim==3.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdG7-2DfsImK"
   },
   "source": [
    "## Define parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_XPN1hGrbOn"
   },
   "outputs": [],
   "source": [
    "HOMEDIR = \"./\"\n",
    "DATADIR = f\"{HOMEDIR}/\"\n",
    "pre_trained_model_path = \"bert-base-multilingual-uncased\"\n",
    "langs = \"en\"  # \"en\" # \"**\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWZyJRWestjq"
   },
   "source": [
    "## Setup Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YX3Q9nBOrNfd"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.debugger import set_trace\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    BertForTokenClassification,\n",
    "    BertTokenizerFast,\n",
    "    Pipeline,\n",
    "    RobertaTokenizerFast,\n",
    "    TokenClassificationPipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.tokenization_utils_base import (\n",
    "    BatchEncoding,\n",
    "    PaddingStrategy,\n",
    "    PreTrainedTokenizerBase,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvOWiSanrNfe",
    "outputId": "acd75965-63b3-401b-c005-554a19624287"
   },
   "outputs": [],
   "source": [
    "{\n",
    "    l.split(\":\")[0]: i + 1\n",
    "    for i, l in enumerate(\n",
    "        \"\"\"ADJ: adjective\n",
    "ADP: adposition\n",
    "ADV: adverb\n",
    "AUX: auxiliary\n",
    "CCONJ: coordinating conjunction\n",
    "DET: determiner\n",
    "INTJ: interjection\n",
    "NOUN: noun\n",
    "NUM: numeral\n",
    "PART: particle\n",
    "PRON: pronoun\n",
    "PROPN: proper noun\n",
    "PUNCT: punctuation\n",
    "SCONJ: subordinating conjunction\n",
    "SYM: symbol\n",
    "VERB: verb\n",
    "X: other\"\"\".splitlines()\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfGAuJwyrNfe"
   },
   "outputs": [],
   "source": [
    "URL_REGEX = re.compile(r\"^http[s]?://[^ ]+\")\n",
    "\n",
    "\n",
    "def clean_tokens(token):\n",
    "    if ord(token[0]) == 65039:\n",
    "        token = token[1:]\n",
    "    if token == chr(65039):\n",
    "        return \"\"\n",
    "    if token == \"\\n\":\n",
    "        return \"[LF]\"\n",
    "    return URL_REGEX.sub(\"[URL]\", token)\n",
    "\n",
    "\n",
    "def read_ud_data(file_path, label_col=3):\n",
    "    with open(file_path) as fp:\n",
    "        all_tokens = []\n",
    "        all_labels = []\n",
    "        for line in tqdm(fp):\n",
    "            seq = json.loads(line)\n",
    "            seq = seq[\"tokens\"]\n",
    "            if not seq:\n",
    "                print(f\"Error: {seq}\")\n",
    "                continue\n",
    "            tokens, labels = zip(*[(t[1], t[label_col]) for t in seq])\n",
    "            tokens = [clean_tokens(token) for token in tokens]\n",
    "            # Remove empty tokens\n",
    "            tokens, labels = tuple(\n",
    "                zip(*[(t, l) for t, l in zip(tokens, labels) if t and t.strip()])\n",
    "            )\n",
    "            all_tokens.append(tokens)\n",
    "            all_labels.append(labels)\n",
    "    return all_tokens, all_labels\n",
    "\n",
    "\n",
    "label2id = {\n",
    "    \"ADJ\": 1,\n",
    "    \"ADP\": 2,\n",
    "    \"ADV\": 3,\n",
    "    \"AUX\": 4,\n",
    "    \"CCONJ\": 5,\n",
    "    \"DET\": 6,\n",
    "    \"INTJ\": 7,\n",
    "    \"NOUN\": 8,\n",
    "    \"NUM\": 9,\n",
    "    \"PART\": 10,\n",
    "    \"PRON\": 11,\n",
    "    \"PROPN\": 12,\n",
    "    \"PUNCT\": 13,\n",
    "    \"SCONJ\": 14,\n",
    "    \"SYM\": 15,\n",
    "    \"VERB\": 16,\n",
    "    \"X\": 17,\n",
    "}\n",
    "\n",
    "LABEL_MAP = {k: k for k in label2id}\n",
    "label2id = {LABEL_MAP[k]: v - 1 for k, v in label2id.items()}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8M82PuHrNfg"
   },
   "outputs": [],
   "source": [
    "class SplitTokenClassificationPipeline(TokenClassificationPipeline):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def __call__(self, inputs: Union[str, List[str]], **kwargs):\n",
    "        \"\"\"\n",
    "    Classify each token of the text(s) given as inputs.\n",
    "    Args:\n",
    "        inputs (:obj:`str` or :obj:`List[str]`):\n",
    "            One or several texts (or one list of texts) for token classification.\n",
    "    Return:\n",
    "        A list or a list of list of :obj:`dict`: Each result comes as a list of dictionaries (one for each token in\n",
    "        the corresponding input, or each entity if this pipeline was instantiated with\n",
    "        :obj:`grouped_entities=True`) with the following keys:\n",
    "        - **word** (:obj:`str`) -- The token/word classified.\n",
    "        - **score** (:obj:`float`) -- The corresponding probability for :obj:`entity`.\n",
    "        - **entity** (:obj:`str`) -- The entity predicted for that token/word.\n",
    "        - **index** (:obj:`int`, only present when ``self.grouped_entities=False``) -- The index of the\n",
    "          corresponding token in the sentence.\n",
    "    \"\"\"\n",
    "        #     set_trace()\n",
    "        inputs, offset_mappings = self._args_parser(\n",
    "            inputs, **kwargs\n",
    "        )  # offset_mappings introduced in newer version\n",
    "        # inputs = self._args_parser(inputs, **kwargs) # offset_mappings introduced in newer version\n",
    "\n",
    "        answers = []\n",
    "\n",
    "        for i, sentence in enumerate(\n",
    "            inputs[0]\n",
    "        ):  # Another addition to only select first list item in newer version\n",
    "            # sentence = sentence[0]\n",
    "            # Manage correct placement of the tensors\n",
    "            with self.device_placement():\n",
    "\n",
    "                tokens = self.tokenizer(\n",
    "                    sentence,\n",
    "                    return_attention_mask=False,\n",
    "                    return_tensors=self.framework,\n",
    "                    truncation=True,\n",
    "                    return_special_tokens_mask=True,\n",
    "                    return_offsets_mapping=self.tokenizer.is_fast,\n",
    "                    is_split_into_words=True,  # This is the new addition,\n",
    "                    padding=True,\n",
    "                    max_length=self.tokenizer.max_len,\n",
    "                )\n",
    "                if self.tokenizer.is_fast:\n",
    "                    offset_mapping = tokens.pop(\"offset_mapping\").cpu().numpy()[0]\n",
    "                elif offset_mappings:\n",
    "                    offset_mapping = offset_mappings[i]\n",
    "                else:\n",
    "                    offset_mapping = None\n",
    "\n",
    "                special_tokens_mask = tokens.pop(\"special_tokens_mask\").cpu().numpy()[0]\n",
    "\n",
    "                # Forward\n",
    "                if self.framework == \"tf\":\n",
    "                    entities = self.model(tokens.data)[0][0].numpy()\n",
    "                    input_ids = tokens[\"input_ids\"].numpy()[0]\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        tokens = self.ensure_tensor_on_device(**tokens)\n",
    "                        entities = self.model(**tokens)[0][0].cpu().numpy()\n",
    "                        input_ids = tokens[\"input_ids\"].cpu().numpy()[0]\n",
    "\n",
    "            score = np.exp(entities) / np.exp(entities).sum(-1, keepdims=True)\n",
    "            labels_idx = score.argmax(axis=-1)\n",
    "\n",
    "            entities = []\n",
    "            # Filter to labels not in `self.ignore_labels`\n",
    "            # Filter special_tokens\n",
    "            filtered_labels_idx = [\n",
    "                (idx, label_idx)\n",
    "                for idx, label_idx in enumerate(labels_idx)\n",
    "                if (self.model.config.id2label[label_idx] not in self.ignore_labels)\n",
    "                and not special_tokens_mask[idx]\n",
    "            ]\n",
    "\n",
    "            for idx, label_idx in filtered_labels_idx:\n",
    "                if offset_mapping is not None:\n",
    "                    start_ind, end_ind = offset_mapping[idx]\n",
    "                    word_ref = sentence[start_ind:end_ind]\n",
    "                    word = self.tokenizer.convert_ids_to_tokens([int(input_ids[idx])])[\n",
    "                        0\n",
    "                    ]\n",
    "                    is_subword = len(word_ref) != len(word)\n",
    "\n",
    "                    if int(input_ids[idx]) == self.tokenizer.unk_token_id:\n",
    "                        word = word_ref\n",
    "                        is_subword = False\n",
    "                else:\n",
    "                    word = self.tokenizer.convert_ids_to_tokens(int(input_ids[idx]))\n",
    "\n",
    "                entity = {\n",
    "                    \"word\": word,\n",
    "                    \"score\": score[idx][label_idx].item(),\n",
    "                    \"entity\": self.model.config.id2label[label_idx],\n",
    "                    \"index\": idx,\n",
    "                    \"offset\": (start_ind, end_ind),\n",
    "                }\n",
    "\n",
    "                entity[\"is_subword\"] = is_subword  # Another addition\n",
    "\n",
    "                if self.grouped_entities and self.ignore_subwords:\n",
    "                    entity[\"is_subword\"] = is_subword\n",
    "\n",
    "                entities += [entity]\n",
    "\n",
    "            if self.grouped_entities:\n",
    "                answers += [self.group_entities(entities)]\n",
    "            # Append ungrouped entities\n",
    "            else:\n",
    "                answers += [entities]\n",
    "\n",
    "        if len(answers) == 1:\n",
    "            return answers[0]\n",
    "        return answers\n",
    "\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(all_preds, all_labels, return_report=False, extra_label_map=None):\n",
    "    if extra_label_map is None:\n",
    "        extra_label_map = {}\n",
    "\n",
    "    def clean_pred(preds):\n",
    "        return [p[\"entity\"] for p in preds if p[\"offset\"][0] == 0]\n",
    "\n",
    "    def clean_label(labels):\n",
    "        return [LABEL_MAP[extra_label_map.get(l, l)] for l in labels]\n",
    "\n",
    "    true_predictions = [clean_pred(preds) for preds in all_preds]\n",
    "    true_labels = [clean_label(labels) for labels in all_labels]\n",
    "    tp = []\n",
    "    tl = []\n",
    "    num_errors = 0\n",
    "    for i, (p, l) in enumerate(zip(true_predictions, true_labels)):\n",
    "        if len(p) != len(l):\n",
    "            # print(f\"{i} len(p)[{len(p)}] != len(l)[{len(l)}], p={p}, l={l}\")\n",
    "            num_errors += 1\n",
    "            continue\n",
    "        tp.append(p)\n",
    "        tl.append(l)\n",
    "\n",
    "    print(f\"Found {num_errors} errors in length mismatch.\")\n",
    "    true_predictions = tp\n",
    "    true_labels = tl\n",
    "\n",
    "    if return_report:\n",
    "        #       report = classification_report(true_labels, true_predictions)\n",
    "        #       print(report)\n",
    "        report = classification_report(\n",
    "            sum(true_labels, []), sum(true_predictions, []), output_dict=True\n",
    "        )\n",
    "        return report\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy_score\": accuracy_score(true_labels, true_predictions),\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def get_preds(all_tokens, ner_pipeline):\n",
    "    all_preds = []\n",
    "    batch_size = 64\n",
    "    for i in trange(0, len(all_tokens) + batch_size, batch_size):\n",
    "        batch = all_tokens[i : i + batch_size]\n",
    "        if batch:\n",
    "            preds = ner_pipeline(batch)\n",
    "            all_preds.extend(preds)\n",
    "    return all_preds\n",
    "\n",
    "\n",
    "def run_eval(data_path, ner_pipeline, extra_label_map=None):\n",
    "    all_tokens, all_labels = read_ud_data(data_path)\n",
    "    all_preds = get_preds(all_tokens, ner_pipeline)\n",
    "    report = compute_metrics(\n",
    "        all_preds, all_labels, return_report=True, extra_label_map=extra_label_map\n",
    "    )\n",
    "    df_report = pd.DataFrame(report).T  # pd.DataFrame.from_dict(report, orient=\"index\")\n",
    "    return df_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UK_nC9ITrNfh"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForTokenClassification:\n",
    "    \"\"\"\n",
    "    NOTE: Code taken from huggingface transformers library\n",
    "    Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "    Args:\n",
    "        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        label_pad_token_id (:obj:`int`, `optional`, defaults to -100):\n",
    "            The id to use when padding the labels (-100 will be automatically ignore by PyTorch loss functions).\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    label_pad_token_id: int = -100\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = (\n",
    "            [feature[label_name] for feature in features]\n",
    "            if label_name in features[0].keys()\n",
    "            else None\n",
    "        )\n",
    "        for feature in features:\n",
    "            feature.pop(label_name)\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n",
    "            return_tensors=\"pt\" if labels is None else None,\n",
    "        )\n",
    "\n",
    "        if labels is None:\n",
    "            return batch\n",
    "\n",
    "        sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "        if padding_side == \"right\":\n",
    "            batch[\"labels\"] = [\n",
    "                label.tolist()\n",
    "                + [self.label_pad_token_id] * (sequence_length - len(label))\n",
    "                for label in labels\n",
    "            ]\n",
    "        else:\n",
    "            batch[\"labels\"] = [\n",
    "                [self.label_pad_token_id] * (sequence_length - len(label))\n",
    "                + label.tolist()\n",
    "                for label in labels\n",
    "            ]\n",
    "\n",
    "        batch = {\n",
    "            k: torch.tensor(v, dtype=torch.int64)[:, : self.max_length]\n",
    "            for k, v in batch.items()\n",
    "        }\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibHYCRP5rNfi"
   },
   "outputs": [],
   "source": [
    "def encode_labels(labels, encodings):\n",
    "    labels = [label2id[LABEL_MAP[label]] for label in labels]\n",
    "    offset = encodings[\"offset_mapping\"]\n",
    "    # create an empty array of -100\n",
    "    n = len(offset)\n",
    "    doc_enc_labels = np.ones(n, dtype=int) * -100\n",
    "    arr_offset = np.array(offset)\n",
    "    positions = np.arange(n)\n",
    "    mask = (arr_offset[:, 0] == 0) & (\n",
    "        (positions != 0) & (positions != n - 1)\n",
    "    )\n",
    "    # set labels whose first offset position is 0 and the second is not 0\n",
    "    doc_enc_labels[mask] = labels\n",
    "    return doc_enc_labels.tolist()\n",
    "\n",
    "\n",
    "class UDDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_paths, tokenizer):\n",
    "        self.file_paths = file_paths if isinstance(file_paths, list) else [file_paths]\n",
    "        self.tokenizer = tokenizer\n",
    "        self._setup()\n",
    "\n",
    "    def _setup(self):\n",
    "        all_tokens, all_labels = [], []\n",
    "        for file_path in self.file_paths:\n",
    "            all_tokens_fp, all_labels_fp = read_ud_data(file_path)\n",
    "            all_tokens += all_tokens_fp\n",
    "            all_labels += all_labels_fp\n",
    "        self.data = []\n",
    "        num_errors = 0\n",
    "        all_encodings = encodings = self.tokenizer(\n",
    "            all_tokens, is_split_into_words=True, return_offsets_mapping=True\n",
    "        )\n",
    "\n",
    "        for i, (tokens, labels) in tqdm(enumerate(zip(all_tokens, all_labels))):\n",
    "            if len(tokens) == 0 or len(labels) == 0:\n",
    "                num_errors += 1\n",
    "                continue\n",
    "            try:\n",
    "                encodings = {k: all_encodings[k][i] for k in all_encodings}\n",
    "                labels = encode_labels(labels, encodings)\n",
    "                encodings.pop(\"offset_mapping\")  # Don't pass to model\n",
    "                self.data.append((encodings, labels))\n",
    "            except ValueError as e:\n",
    "                num_errors += 1\n",
    "                print(f\"idx={i} has issues [num_errors={num_errors}/{i}]: {e}\")\n",
    "                continue\n",
    "        print(\n",
    "            f\"Errors: {num_errors}, data={len(self.data)}, %error={num_errors*100./len(self.data)}\"\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encodings, labels = self.data[idx]\n",
    "        item = {key: torch.tensor(val) for key, val in encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(labels)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09vZMOYIrNfj"
   },
   "source": [
    "## Run Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWAvV7RdrNfk"
   },
   "outputs": [],
   "source": [
    "model_dir = pre_trained_model_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    str(model_dir), max_len=512, truncation=True, padding=True, use_fast=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiL6uYbcrNfk",
    "outputId": "9e61c48f-2a48-4ec2-f6bd-ce7bd9faeb5b"
   },
   "outputs": [],
   "source": [
    "data_dir = Path(f\"{HOMEDIR}/ud_data/\").expanduser()\n",
    "\n",
    "LANG_MAP = {\n",
    "    \"en\": \"English\",\n",
    "    \"ja\": \"Japanese\",\n",
    "    \"ar\": \"Arabic\",\n",
    "    \"hi\": \"Hindi\",\n",
    "    \"**\": \"**\",\n",
    "}\n",
    "\n",
    "langs = \"en\"  # \"en\" # \"**\"\n",
    "train_data_path = list(\n",
    "    data_dir.glob(f\"./UD_{LANG_MAP.get(langs, langs)}-*/*-ud-train.conllu.json\")\n",
    ")\n",
    "val_data_path = list(\n",
    "    data_dir.glob(f\"./UD_{LANG_MAP.get(langs, langs)}-*/*-ud-dev.conllu.json\")\n",
    ")\n",
    "test_data_path = list(\n",
    "    data_dir.glob(f\"./UD_{LANG_MAP.get(langs, langs)}-*/*-ud-test.conllu.json\")\n",
    ")\n",
    "train_data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAWN1laZrNfl",
    "outputId": "96bc7a0a-6131-4b62-abe4-b38f82d7474a"
   },
   "outputs": [],
   "source": [
    "train_dataset = UDDataset(train_data_path, tokenizer)\n",
    "val_dataset = UDDataset(val_data_path, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGrlYnTerNfl",
    "outputId": "04dbe9aa-951e-4d50-83e5-cbae62d01973"
   },
   "outputs": [],
   "source": [
    "len(train_dataset), len(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9VriPiGrNfl",
    "outputId": "95eaefe0-4b59-4bc8-edcd-4dbe43014520"
   },
   "outputs": [],
   "source": [
    "model_prefix = \"multi\" if langs == \"**\" else langs\n",
    "ner_model_dir = str(Path(f\"{HOMEDIR}/{model_prefix}_udpos_model\").expanduser())\n",
    "logging_dir = str(Path(f\"{HOMEDIR}/{model_prefix}_udpos_logs\").expanduser())\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, padding=True, max_length=tokenizer.max_len\n",
    ")\n",
    "\n",
    "eval_every_steps = -1  # -1 for eval at end\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(ner_model_dir),  # output directory\n",
    "    num_train_epochs=3,  # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    logging_dir=str(logging_dir),  # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=100,\n",
    "    save_steps=500 if eval_every_steps < 1 else eval_every_steps,\n",
    "    save_total_limit=2 if eval_every_steps < 1 else None,\n",
    "    max_steps=-1 if eval_every_steps < 1 else 5,\n",
    "    label_names=[id2label[i] for i in range(len(id2label))],\n",
    ")\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        str(model_dir), num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,  # the instantiated 🤗 Transformers model to be trained\n",
    "        args=training_args,  # training arguments, defined above\n",
    "        train_dataset=train_dataset,  # training dataset\n",
    "        eval_dataset=val_dataset,  # evaluation dataset\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(ner_model_dir)\n",
    "    tokenizer.save_pretrained(ner_model_dir)\n",
    "    return model, trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PbFLtLWYrNfm",
    "outputId": "bf5e1d55-7a90-44a2-d53e-162ee688f1f9"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model, trainer = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXYqsnEarNfn"
   },
   "source": [
    "## Run evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbhwKG5PrNfn"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import trange\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    Pipeline,\n",
    "    TokenClassificationPipeline,\n",
    "    pipeline,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8CDvWOcrNfo"
   },
   "outputs": [],
   "source": [
    "ner_model_dir = str(Path(f\"{HOMEDIR}/{model_prefix}_udpos_model/\").expanduser())\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(ner_model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    str(model_dir), max_len=512, truncation=True, padding=True, use_fast=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kd2R8XH3rNfo"
   },
   "outputs": [],
   "source": [
    "ner_pipeline = SplitTokenClassificationPipeline(\n",
    "    model=model, tokenizer=tokenizer, grouped_entities=False, ignore_labels=[], device=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJUttxTwrNfo"
   },
   "outputs": [],
   "source": [
    "data_dir = Path(f\"{HOMEDIR}/ud_data/\").expanduser()\n",
    "\n",
    "LANG_MAP = {\"en\": \"English\", \"ja\": \"Japanese\", \"ar\": \"Arabic\", \"hi\": \"Hindi\", \"**\": \"*\"}\n",
    "\n",
    "langs = \"**\"  # \"en\" # \"**\"\n",
    "train_data_path = list(\n",
    "    data_dir.glob(f\"./UD_{LANG_MAP.get(langs, langs)}-*/*-ud-train.conllu.json\")\n",
    ")\n",
    "val_data_path = list(\n",
    "    data_dir.glob(f\"./UD_{LANG_MAP.get(langs, langs)}-*/*-ud-dev.conllu.json\")\n",
    ")\n",
    "test_data_path = list(\n",
    "    data_dir.glob(f\"./UD_{LANG_MAP.get(langs, langs)}-*/*-ud-test.conllu.json\")\n",
    ")\n",
    "extra_label_map = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBg1VE8BrNfo",
    "outputId": "a08f6436-ef25-4f21-fe40-7bb3514b19c7"
   },
   "outputs": [],
   "source": [
    "test_data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-USbwuPorNfp",
    "outputId": "a2607e4e-a464-49e3-d1ff-293ed5179f46"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "reports = {}\n",
    "for test_path in test_data_path:\n",
    "    lang, split = test_path.name.split(\"-\")[0].split(\"_\")\n",
    "    print(lang, split, test_path.name)\n",
    "    report = run_eval(test_path, ner_pipeline, extra_label_map=extra_label_map)\n",
    "    reports[(lang, split)] = report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvvUjhUfrNfp",
    "outputId": "5d8bb4dc-86ba-483f-a517-f5897dde3d0b"
   },
   "outputs": [],
   "source": [
    "report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fR_2FwRQrNfp",
    "outputId": "47af36b9-47c7-456d-b1c6-9b7b2150e92c"
   },
   "outputs": [],
   "source": [
    "df_report = pd.concat(reports)\n",
    "df_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TPy0o2nrNfq"
   },
   "outputs": [],
   "source": [
    "df_report.to_csv(Path(ner_model_dir) / \"test_eval_report.txt\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Oa09WCjrNfq",
    "outputId": "9bc5d46c-85a8-4a7a-8386-5d56951d857f"
   },
   "outputs": [],
   "source": [
    "df_report[df_report.index.isin([\"accuracy\"], level=2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TjYpFr5NrNfq",
    "outputId": "673a3043-349c-4f67-d31a-42ba2e3eb36b"
   },
   "outputs": [],
   "source": [
    "df_report = pd.read_csv(\n",
    "    Path(ner_model_dir) / \"test_eval_report.txt\", sep=\"\\t\", index_col=[0, 1, 2]\n",
    ")\n",
    "df_report[df_report.index.isin([\"accuracy\"], level=2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiHjNbaZrNfq",
    "outputId": "392db712-cf53-48f7-9c9d-d58789429238"
   },
   "outputs": [],
   "source": [
    "if Path(f\"{HOMEDIR}/test_eval_report.txt\").expanduser().exists():\n",
    "    df_report = pd.read_csv(\n",
    "        f\"{HOMEDIR}/test_eval_report.txt\", sep=\"\\t\", index_col=[0, 1]\n",
    "    )\n",
    "df_report[df_report.index.isin([\"accuracy\"], level=2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2s8pQPMrNfq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Finetune_UD_POS.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mldash_entity": {
   "created_at_millis": 1612222556594,
   "hash": "8fc3d0d3d4f867d5abe5fb183d17ca12bf8b1e4e",
   "inferred_pdp_safe": true,
   "is_vfs_dir": false,
   "marked_pdp_safe": false,
   "owner": "smishra",
   "shared_to_everyone": false,
   "shared_to_ldap_groups": [],
   "shared_to_ldap_users": [],
   "size": 13251,
   "tags": [],
   "uuid": "1356385828520357889",
   "vfs_path": "/user/smishra/notebooks/NLPLib/Finetune_UD_POS.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
