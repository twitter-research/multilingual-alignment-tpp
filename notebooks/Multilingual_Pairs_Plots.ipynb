{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "// Copyright 2020 Twitter, Inc.\n",
    "// SPDX-License-Identifier: Apache-2.0\n",
    "```\n",
    "\n",
    "# Make plots and tables for the paper\n",
    "\n",
    "Make plots and tables for the paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Fjip4YztNIh"
   },
   "source": [
    "## Setup libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79yDGUm8s66W"
   },
   "outputs": [],
   "source": [
    "%pip install transformers==3.5.1 datasets==1.1.2 torch==1.4.0 seqeval==1.2.2 gensim==3.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tensorflow.io.gfile import GFile\n",
    "from tensorflow.io.gfile import glob as Glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACT_TASK_CONFIG = re.compile(\n",
    "    r\"mbert_model_ft_(?P<seq>[a-z_]+?)_en_(?P<langs>[a-z_]+?)_2t_bce\"\n",
    ")\n",
    "EXTRACT_TASK_CONFIG.match(\"mbert_model_ft_tt_en_hi_2t_bce\").groupdict()\n",
    "BASE_DIR = \"models/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = {}\n",
    "df_reports = {}\n",
    "for path in (\n",
    "    list(\n",
    "        Glob(\n",
    "            f\"{BASE_DIR}/mbert_model_ft_*_2t_bce/en_ner_model/test_eval_report*_all_checkpoints.txt\"\n",
    "        )\n",
    "    )\n",
    "    + list(\n",
    "        Glob(\n",
    "            f\"{BASE_DIR}/mbert_model/en_ner_model/test_eval_report_*_all_checkpoints.txt\"\n",
    "        )\n",
    "    )\n",
    "    + list(\n",
    "        Glob(\n",
    "            f\"{BASE_DIR}/mbert_model_tt_*_2t_bce/en_ner_model/*test_eval_report_all_checkpoints.txt\"\n",
    "        )\n",
    "    )\n",
    "):\n",
    "    dirname = Path(path).parts[-3]\n",
    "    filename = Path(path).name\n",
    "    if dirname == \"mbert_model\":\n",
    "        config = {\"seq\": \"mbert\", \"langs\": \"en\"}\n",
    "    elif \"mbert_model_tt_\" in dirname:\n",
    "        config = {\"seq\": \"all\", \"langs\": \"en\"}\n",
    "        if \"equal\" in dirname:\n",
    "            config[\"seq\"] = \"all_equal\"\n",
    "    else:\n",
    "        config = {}\n",
    "    match = EXTRACT_TASK_CONFIG.match(dirname)\n",
    "    if match:\n",
    "        config = match.groupdict()\n",
    "\n",
    "    ft = \"ft\" if config[\"seq\"] != \"mbert\" else \"base\"\n",
    "    lang = filename.split(\"_\")[3]\n",
    "\n",
    "    print(path, config, lang)\n",
    "\n",
    "    with GFile(path) as fp:\n",
    "        df_all_reports = pd.read_csv(fp, sep=\"\\t\", index_col=[0, 1, 2])\n",
    "    df_reports[(lang, ft)] = df_all_reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (lang, ft), df_all_reports in df_reports.items():\n",
    "    df_table[(lang, ft)] = (\n",
    "        df_all_reports[df_all_reports.index.isin([\"micro avg\"], level=2)]\n",
    "        .drop([\"support\", \"precision\", \"recall\"], axis=1)\n",
    "        .reset_index(drop=True)\n",
    "        .rename(columns={\"precision\": \"P\", \"recall\": \"R\", \"f1-score\": \"F1\"})[\"F1\"]\n",
    "    )\n",
    "\n",
    "\n",
    "df_table = pd.concat(df_table, axis=1).sort_index(axis=1)[[\"hi\", \"ja\", \"ar\"]] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"precision\", 1):\n",
    "    display(df_table)\n",
    "    print(df_table.to_latex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, sharex=True, sharey=False, figsize=(5, 4))\n",
    "for (lang, ft), df_all_reports in df_reports.items():\n",
    "    i = 0 if ft == \"base\" else 1\n",
    "    print(lang, ft)\n",
    "    display(df_all_reports[df_all_reports.index.isin([\"micro avg\"], level=2)])\n",
    "    df_t = df_all_reports[df_all_reports.index.isin([\"micro avg\"], level=2)][\n",
    "        \"f1-score\"\n",
    "    ].reset_index()[\"f1-score\"]\n",
    "    ((df_t - df_t.iloc[0]) * 100 / df_t.iloc[0]).plot(marker=\"o\", label=lang, ax=ax[i])\n",
    "\n",
    "ax[0].set_title(\"mbert\")\n",
    "ax[1].set_title(\"finetuned\")\n",
    "\n",
    "ax[1].set_xlabel(\"iteration\")\n",
    "\n",
    "ax[0].set_ylabel(\"$\\%\\Delta F_1$\")\n",
    "ax[1].set_ylabel(\"$\\%\\Delta F_1$\")\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = {}\n",
    "for path in (\n",
    "    list(Glob(f\"{BASE_DIR}/mbert_model_ft_*_2t_bce/en_ner_model/*test_eval_report.txt\"))\n",
    "    + list(Glob(f\"{BASE_DIR}/mbert_model/en_ner_model/*test_eval_report.txt\"))\n",
    "    + list(\n",
    "        Glob(f\"{BASE_DIR}/mbert_model_tt_*_2t_bce/en_ner_model/*test_eval_report.txt\")\n",
    "    )\n",
    "):\n",
    "    dirname = Path(path).parts[-3]\n",
    "    filename = Path(path).name\n",
    "    if dirname == \"mbert_model\":\n",
    "        config = {\"seq\": \"mbert\", \"langs\": \"en\"}\n",
    "    elif \"mbert_model_tt_\" in dirname:\n",
    "        config = {\"seq\": \"all\", \"langs\": \"en\"}\n",
    "        if \"equal\" in dirname:\n",
    "            config[\"seq\"] = \"all_equal\"\n",
    "    else:\n",
    "        config = {}\n",
    "    match = EXTRACT_TASK_CONFIG.match(dirname)\n",
    "    if match:\n",
    "        config = match.groupdict()\n",
    "    reporttype = \"SSEA\" if filename.startswith(\"ssea\") else \"BASE\"\n",
    "    with GFile(path) as fp:\n",
    "        df_report = pd.read_csv(fp, sep=\"\\t\", index_col=[0, 1])\n",
    "    print(path, reporttype, config)\n",
    "    lang = config[\"langs\"]\n",
    "    seq = config[\"seq\"]\n",
    "\n",
    "    df_table[(lang, seq, reporttype)] = (\n",
    "        df_report[df_report.index.isin([\"micro avg\"], level=1)]\n",
    "        .drop([\"support\", \"precision\", \"recall\"], axis=1)\n",
    "        .rename(columns={\"precision\": \"P\", \"recall\": \"R\", \"f1-score\": \"F1\"})[\"F1\"]\n",
    "        .reset_index(level=1, drop=True)\n",
    "    )\n",
    "\n",
    "    display(df_table[(lang, seq, reporttype)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report[df_report.index.isin([\"micro avg\"], level=1)].drop(\n",
    "    [\"support\", \"precision\", \"recall\"], axis=1\n",
    ").rename(columns={\"precision\": \"P\", \"recall\": \"R\", \"f1-score\": \"F1\"})[\"F1\"].reset_index(\n",
    "    level=1, drop=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = pd.concat(df_table, axis=1).sort_index(\n",
    "    axis=1\n",
    ")\n",
    "with pd.option_context(\"precision\", 3):\n",
    "    display(df_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_langs = {}\n",
    "for lang in {\"hi\", \"ar\", \"ja\"}:\n",
    "    df_table_langs[lang] = df_table.T.loc[lang, lang]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.T.loc[\"en\", [\"ja\", \"ar\", \"hi\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.concat(\n",
    "    [pd.concat(df_table_langs, axis=1), df_table.T.loc[\"en\", [\"ja\", \"ar\", \"hi\"]]],\n",
    "    axis=0,\n",
    ").max(level=0)\n",
    "df_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.loc[[\"tt_wd\", \"tt_wm\", \"wm_tt\", \"wm_wd\"], [\"hi\", \"ja\", \"ar\"]].max().to_frame().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_order = [\"hi\", \"ja\", \"ar\"]\n",
    "df_tt = pd.concat(\n",
    "    [\n",
    "        df_t.loc[[\"mbert\"], lang_order],\n",
    "        df_t.loc[[\"tt\", \"wd\", \"wm\"], lang_order],\n",
    "        df_t.loc[[\"tt\", \"wd\", \"wm\"], lang_order]\n",
    "        .max()\n",
    "        .to_frame()\n",
    "        .T.rename(index={0: \"one\"}),\n",
    "        df_t.loc[[\"tt_wd\", \"tt_wm\", \"wm_tt\", \"wm_wd\"], lang_order]\n",
    "        .max()\n",
    "        .to_frame()\n",
    "        .T.rename(index={0: \"pair\"}),\n",
    "        df_t.loc[[\"tt_wd_wm\", \"tt_wm_wd\", \"wm_wd_tt\"], lang_order]\n",
    "        .max()\n",
    "        .to_frame()\n",
    "        .T.rename(index={0: \"triple\"}),\n",
    "        df_t.loc[[\"all\", \"all_equal\"], lang_order],\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "df_tt = df_tt * 100\n",
    "\n",
    "with pd.option_context(\"precision\", 1):\n",
    "    display(df_tt)\n",
    "    print(df_tt.to_latex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt_with_percent = (\n",
    "    pd.concat({\"F1\": df_tt, \"imp %\": ((df_tt / df_tt.loc[\"mbert\"]) - 1) * 100}, axis=1)\n",
    "    .reorder_levels([1, 0], axis=1)\n",
    "    .sort_index(axis=1)[[\"hi\", \"ja\", \"ar\"]]\n",
    ")\n",
    "\n",
    "\n",
    "with pd.option_context(\"precision\", 1):\n",
    "    display(df_tt_with_percent)\n",
    "    print(df_tt_with_percent.to_latex())\n",
    "    display(df_tt_with_percent.loc[[\"one\", \"pair\", \"triple\"], :].max().to_frame().T)\n",
    "    print(\n",
    "        df_tt_with_percent.loc[[\"one\", \"pair\", \"triple\"], :]\n",
    "        .max()\n",
    "        .to_frame()\n",
    "        .T.to_latex()\n",
    "    )\n",
    "    display(df_tt_with_percent.loc[[\"one\", \"pair\"], :].max().to_frame().T)\n",
    "    print(df_tt_with_percent.loc[[\"one\", \"pair\"], :].max().to_frame().T.to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = {}\n",
    "for path in (\n",
    "    list(\n",
    "        Glob(\n",
    "            f\"{BASE_DIR}/mbert_model_ft_*_2t_bce/en_sentiment_model/*test_eval_report.txt\"\n",
    "        )\n",
    "    )\n",
    "    + list(Glob(f\"{BASE_DIR}/mbert_model/en_sentiment_model/*test_eval_report.txt\"))\n",
    "    + list(\n",
    "        Glob(\n",
    "            f\"{BASE_DIR}/mbert_model_tt_*_2t_bce/en_sentiment_model/*test_eval_report.txt\"\n",
    "        )\n",
    "    )\n",
    "):\n",
    "    dirname = Path(path).parts[-3]\n",
    "    filename = Path(path).name\n",
    "    if dirname == \"mbert_model\":\n",
    "        config = {\"seq\": \"mbert\", \"langs\": \"en\"}\n",
    "    elif \"en_hi_en_ja_en_ar\" in dirname:\n",
    "        config = {\"seq\": \"all\", \"langs\": \"en\"}\n",
    "        if \"equal_2t_bce\" in dirname:\n",
    "            config[\"seq\"] = \"all_equal\"\n",
    "    else:\n",
    "        config = {}\n",
    "    match = EXTRACT_TASK_CONFIG.match(dirname)\n",
    "    if match:\n",
    "        config = match.groupdict()\n",
    "    reporttype = \"SSEA\" if filename.startswith(\"ssea\") else \"BASE\"\n",
    "    with GFile(path) as fp:\n",
    "        # df_report = pd.read_csv(fp, sep=\"\\t\", index_col=[0, 1])\n",
    "        df_report = pd.read_csv(fp, sep=\"\\t\", index_col=[0, 1, 2])\n",
    "    print(path, reporttype, config)\n",
    "    lang = config[\"langs\"]\n",
    "    seq = config[\"seq\"]\n",
    "\n",
    "    df_table[(lang, seq, reporttype)] = (\n",
    "        df_report[df_report.index.isin([\"macro avg\"], level=2)]\n",
    "        .drop([\"support\", \"precision\", \"recall\"], axis=1)\n",
    "        .rename(columns={\"precision\": \"P\", \"recall\": \"R\", \"f1-score\": \"F1\"})[\"F1\"]\n",
    "        .reset_index(level=1, drop=True)\n",
    "        .droplevel(1)\n",
    "    )\n",
    "\n",
    "    display(df_table[(lang, seq, reporttype)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report[df_report.index.isin([\"macro avg\"], level=2)].drop(\n",
    "    [\"support\", \"precision\", \"recall\"], axis=1\n",
    ").rename(columns={\"precision\": \"P\", \"recall\": \"R\", \"f1-score\": \"F1\"})[\"F1\"].reset_index(\n",
    "    level=1, drop=True\n",
    ").droplevel(\n",
    "    1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = pd.concat(df_table, axis=1).sort_index(\n",
    "    axis=1\n",
    ")  # .reorder_levels([2, 0, 1], axis=1)\n",
    "with pd.option_context(\"precision\", 3):\n",
    "    display(df_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.T.loc[\"en\", \"hi\"]  # [\"hi\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_langs = {}\n",
    "for lang in {\"hi\", \"ar\", \"ja\"}:\n",
    "    df_table_langs[lang] = df_table.T.loc[lang, lang]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.concat(\n",
    "    [pd.concat(df_table_langs, axis=1), df_table.T.loc[\"en\", [\"ja\", \"ar\", \"hi\"]]],\n",
    "    axis=0,\n",
    ").max(level=0)\n",
    "df_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_order = [\"hi\", \"ja\", \"ar\"]\n",
    "df_tt = pd.concat(\n",
    "    [\n",
    "        df_t.loc[[\"mbert\"], lang_order],\n",
    "        df_t.loc[\n",
    "            [\"tt\", \"wd\", \"wm\"], lang_order\n",
    "        ],\n",
    "        df_t.loc[[\"tt\", \"wd\", \"wm\"], lang_order]\n",
    "        .max()\n",
    "        .to_frame()\n",
    "        .T.rename(index={0: \"one\"}),\n",
    "        df_t.loc[[\"tt_wd\", \"wm_tt\", \"tt_wm\", \"wm_wd\"], lang_order]\n",
    "        .max()\n",
    "        .to_frame()\n",
    "        .T.rename(index={0: \"pair\"}),\n",
    "        df_t.loc[[\"tt_wd_wm\", \"tt_wm_wd\", \"wm_wd_tt\"], lang_order]\n",
    "        .max()\n",
    "        .to_frame()\n",
    "        .T.rename(index={0: \"triple\"}),\n",
    "        df_t.loc[[\"all\"], lang_order],\n",
    "        df_t.loc[[\"all_equal\"], lang_order],\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "df_tt = df_tt * 100\n",
    "\n",
    "with pd.option_context(\"precision\", 1):\n",
    "    display(df_tt)\n",
    "    print(df_tt.to_latex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt_with_percent = (\n",
    "    pd.concat({\"F1\": df_tt, \"imp %\": ((df_tt / df_tt.loc[\"mbert\"]) - 1) * 100}, axis=1)\n",
    "    .reorder_levels([1, 0], axis=1)\n",
    "    .sort_index(axis=1)[[\"hi\", \"ja\", \"ar\"]]\n",
    ")\n",
    "\n",
    "\n",
    "with pd.option_context(\"display.float_format\", \"{:.1f}\".format):\n",
    "    display(df_tt_with_percent)\n",
    "    print(df_tt_with_percent.to_latex())\n",
    "    display(df_tt_with_percent.loc[[\"one\", \"pair\", \"triple\"], :].max().to_frame().T)\n",
    "    print(\n",
    "        df_tt_with_percent.loc[[\"one\", \"pair\", \"triple\"], :]\n",
    "        .max()\n",
    "        .to_frame()\n",
    "        .T.to_latex()\n",
    "    )\n",
    "    display(df_tt_with_percent.loc[[\"one\", \"pair\"], :].max().to_frame().T)\n",
    "    print(df_tt_with_percent.loc[[\"one\", \"pair\"], :].max().to_frame().T.to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UD POS plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = {}\n",
    "for path in (\n",
    "    list(\n",
    "        Glob(f\"{BASE_DIR}/mbert_model_ft_*_2t_bce/en_udpos_model/*test_eval_report.txt\")\n",
    "    )\n",
    "    + list(Glob(f\"{BASE_DIR}/mbert_model/en_udpos_model/*test_eval_report.txt\"))\n",
    "    + list(\n",
    "        Glob(f\"{BASE_DIR}/mbert_model_tt_*_2t_bce/en_udpos_model/*test_eval_report.txt\")\n",
    "    )\n",
    "):\n",
    "    dirname = Path(path).parts[-3]\n",
    "    filename = Path(path).name\n",
    "    if dirname == \"mbert_model\":\n",
    "        config = {\"seq\": \"mbert\", \"langs\": \"en\"}\n",
    "    elif \"en_hi_en_ja_en_ar\" in dirname:\n",
    "        config = {\"seq\": \"all\", \"langs\": \"en\"}\n",
    "        if \"equal_2t_bce\" in dirname:\n",
    "            config[\"seq\"] = \"all_equal\"\n",
    "    else:\n",
    "        config = {}\n",
    "    match = EXTRACT_TASK_CONFIG.match(dirname)\n",
    "    if match:\n",
    "        config = match.groupdict()\n",
    "    reporttype = \"SSEA\" if filename.startswith(\"ssea\") else \"BASE\"\n",
    "    with GFile(path) as fp:\n",
    "        # df_report = pd.read_csv(fp, sep=\"\\t\", index_col=[0, 1])\n",
    "        df_report = pd.read_csv(fp, sep=\"\\t\", index_col=[0, 1, 2])\n",
    "    print(path, reporttype, config)\n",
    "    lang = config[\"langs\"]\n",
    "    seq = config[\"seq\"]\n",
    "\n",
    "    df_table[(lang, seq, reporttype)] = (\n",
    "        df_report[df_report.index.isin([\"accuracy\"], level=2)]\n",
    "        .drop([\"support\", \"precision\", \"recall\"], axis=1)\n",
    "        .rename(columns={\"precision\": \"P\", \"recall\": \"R\", \"f1-score\": \"F1\"})[\"F1\"]\n",
    "        .reset_index(level=2, drop=True)\n",
    "    )\n",
    "\n",
    "    display(df_table[(lang, seq, reporttype)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report[df_report.index.isin([\"accuracy\"], level=2)].drop(\n",
    "    [\"support\", \"precision\", \"recall\"], axis=1\n",
    ").rename(columns={\"precision\": \"P\", \"recall\": \"R\", \"f1-score\": \"F1\"})[\"F1\"].reset_index(\n",
    "    level=2, drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = pd.concat(df_table, axis=1).sort_index(\n",
    "    axis=1\n",
    ")\n",
    "with pd.option_context(\"precision\", 3):\n",
    "    display(df_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_langs = {}\n",
    "for lang in {\"hi\", \"ar\", \"ja\"}:\n",
    "    df_table_langs[lang] = df_table.T.loc[lang, lang]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = (\n",
    "    pd.concat(\n",
    "        [pd.concat(df_table_langs, axis=1), df_table.T.loc[\"en\", [\"ja\", \"ar\", \"hi\"]]],\n",
    "        axis=0,\n",
    "    )\n",
    "    .max(level=0)\n",
    "    .droplevel(1, axis=1)\n",
    ")\n",
    "df_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_order = [\"hi\", \"ja\", \"ar\"]\n",
    "df_tt = pd.concat(\n",
    "    [\n",
    "        df_t.loc[[\"mbert\"], lang_order],\n",
    "        df_t.loc[\n",
    "            [\"tt\", \"wd\", \"wm\"], lang_order\n",
    "        ],\n",
    "        df_t.loc[[\"tt\", \"wd\", \"wm\"], lang_order]\n",
    "        .max()\n",
    "        .to_frame()\n",
    "        .T.rename(index={0: \"one\"}),\n",
    "        df_t.loc[[\"tt_wd\", \"wm_tt\", \"tt_wm\", \"wm_wd\"], lang_order]\n",
    "        .max()\n",
    "        .to_frame()\n",
    "        .T.rename(index={0: \"pair\"}),\n",
    "        df_t.loc[[\"tt_wd_wm\", \"tt_wm_wd\", \"wm_wd_tt\"], lang_order]\n",
    "        .max()\n",
    "        .to_frame()\n",
    "        .T.rename(index={0: \"triple\"}),\n",
    "        df_t.loc[[\"all\"], lang_order],\n",
    "        df_t.loc[[\"all_equal\"], lang_order],\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "df_tt = df_tt * 100\n",
    "\n",
    "with pd.option_context(\"precision\", 1):\n",
    "    display(df_tt)\n",
    "    print(df_tt.to_latex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt_with_percent = (\n",
    "    pd.concat({\"F1\": df_tt, \"imp %\": ((df_tt / df_tt.loc[\"mbert\"]) - 1) * 100}, axis=1)\n",
    "    .reorder_levels([1, 0], axis=1)\n",
    "    .sort_index(axis=1)[[\"hi\", \"ja\", \"ar\"]]\n",
    ")\n",
    "\n",
    "\n",
    "with pd.option_context(\"display.float_format\", \"{:.1f}\".format):\n",
    "    display(df_tt_with_percent)\n",
    "    print(df_tt_with_percent.to_latex())\n",
    "    display(df_tt_with_percent.loc[[\"one\", \"pair\", \"triple\"], :].max().to_frame().T)\n",
    "    print(\n",
    "        df_tt_with_percent.loc[[\"one\", \"pair\", \"triple\"], :]\n",
    "        .max()\n",
    "        .to_frame()\n",
    "        .T.to_latex()\n",
    "    )\n",
    "    display(df_tt_with_percent.loc[[\"one\", \"pair\"], :].max().to_frame().T)\n",
    "    print(df_tt_with_percent.loc[[\"one\", \"pair\"], :].max().to_frame().T.to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = Path(\"../data/en_ar_embeddings.ft.npz\").expanduser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_data = np.load(embeddings_path)\n",
    "embedding_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = Path(\"../data/en_ar_embeddings.base.npz\").expanduser()\n",
    "embedding_data_base = np.load(embeddings_path)\n",
    "embedding_data_base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_data_base[\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "all_embedding = np.vstack(\n",
    "    [embedding_data_base[\"embeddings\"][:n], embedding_data[\"embeddings\"][:n]]\n",
    ")\n",
    "all_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "low_embed = TSNE().fit_transform(all_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_embed = low_embed.reshape(2, -1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(6, 6))\n",
    "\n",
    "for i in range(0, low_embed.shape[1], 2):\n",
    "    marker = \"o\"  # f\"${i}$\"\n",
    "    ax[0].scatter(\n",
    "        low_embed[0, i : i + 2, 0],\n",
    "        low_embed[0, i : i + 2, 1],\n",
    "        marker=marker,\n",
    "        color=[\"k\", \"r\"],\n",
    "        s=100,\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    ax[1].scatter(\n",
    "        low_embed[1, i : i + 2, 0],\n",
    "        low_embed[1, i : i + 2, 1],\n",
    "        marker=marker,\n",
    "        color=[\"k\", \"r\"],\n",
    "        s=100,\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "ax[0].set_title(\"mBERT\")\n",
    "ax[1].set_title(\"mBERT fine-tuned on all languages\")\n",
    "fig.tight_layout()\n",
    "plt.savefig(Path(\"../figures/en_ar_embeddings.pdf\").expanduser(), bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(6, 3))\n",
    "\n",
    "n = all_embedding.shape[0]//2 \n",
    "print(n)\n",
    "dist_mbert = []\n",
    "low_embed = %time TSNE().fit_transform(all_embedding[:n])\n",
    "for i in range(0, low_embed.shape[0], 2):\n",
    "  marker = \"o\"\n",
    "  ax[0].scatter(low_embed[i:i+2, 0], low_embed[i:i+2, 1], marker=marker, color=[\"k\", \"r\"], s=100, alpha=0.2)\n",
    "  d = ((low_embed[i] - low_embed[i+1])**2).sum()\n",
    "  dist_mbert.append(d)\n",
    "\n",
    "\n",
    "dist_ft = []\n",
    "low_embed = %time TSNE().fit_transform(all_embedding[n:])\n",
    "for i in range(0, low_embed.shape[0], 2):  \n",
    "  ax[1].scatter(low_embed[i:i+2, 0], low_embed[i:i+2, 1], marker=marker, color=[\"k\", \"r\"], s=100, alpha=0.2)\n",
    "  d = ((low_embed[i] - low_embed[i+1])**2).sum()\n",
    "  dist_ft.append(d)\n",
    "  \n",
    "ax[0].set_title(\"mBERT\")\n",
    "ax[1].set_title(\"mBERT + TPP\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].axis(\"off\")\n",
    "fig.tight_layout()\n",
    "plt.savefig(Path(\"../figures/en_ar_embeddings.pdf\").expanduser(), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(6, 3))\n",
    "\n",
    "n = all_embedding.shape[0]//2 \n",
    "print(n)\n",
    "dist_mbert = []\n",
    "low_embed = %time TSNE().fit_transform(all_embedding[:n])\n",
    "markers = [\"o\", \"s\"]\n",
    "colors = [\"k\", \"r\"]\n",
    "langs = [\"EN\", \"AR\"]\n",
    "for i, lang in enumerate(langs):\n",
    "  ax[0].scatter(low_embed[i::2, 0], low_embed[i::2, 1], marker=markers[i], color=colors[i], s=100, alpha=0.2, label=lang)\n",
    "  \n",
    "dist_mbert = np.linalg.norm(low_embed[0::2] - low_embed[1::2], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "dist_ft = []\n",
    "low_embed = %time TSNE().fit_transform(all_embedding[n:])\n",
    "for i, lang in enumerate(langs):\n",
    "  ax[1].scatter(low_embed[i::2, 0], low_embed[i::2, 1], marker=markers[i], color=colors[i], s=100, alpha=0.2, label=lang)\n",
    "  \n",
    "dist_ft = np.linalg.norm(low_embed[0::2] - low_embed[1::2], axis=1)\n",
    "  \n",
    "ax[0].set_title(\"mBERT\")\n",
    "ax[1].set_title(\"mBERT + TPP\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[0].legend()\n",
    "fig.tight_layout()\n",
    "plt.savefig(Path(\"../figures/en_ar_embeddings.pdf\").expanduser(), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mbert = dist_mbert / np.max(dist_mbert)\n",
    "dist_ft = dist_ft / np.max(dist_ft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dist_mbert, dist_ft, linestyle=\"none\", marker=\"o\", color=\"k\")\n",
    "plt.plot([0, 1], [0, 1], color=\"0.5\", linestyle=\"--\", lw=1)\n",
    "ax = plt.gca()\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_xlabel(\"mBERT distance\")\n",
    "ax.set_ylabel(\"mBERT + TPP distance\")\n",
    "plt.savefig(\n",
    "    Path(\"../figures/en_ar_embeddings_dist.pdf\").expanduser(), bbox_inches=\"tight\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_delta_dist = dist_ft - dist_mbert\n",
    "total_delta_dist.mean(), total_delta_dist.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(6, 3))\n",
    "\n",
    "n = all_embedding.shape[0]//2 \n",
    "print(n)\n",
    "dist_mbert = []\n",
    "plot_n = 20\n",
    "low_embed = %time TSNE().fit_transform(all_embedding[:n])[:plot_n]\n",
    "markers = [\"o\", \"s\"]\n",
    "colors = [\"k\", \"r\"]\n",
    "for i, lang in enumerate([\"en\", \"ar\"]):\n",
    "  ax[0].scatter(low_embed[i::2, 0], low_embed[i::2, 1], marker=markers[i], color=colors[i], s=100, alpha=0.2, label=lang)\n",
    "  \n",
    "dist_mbert = np.linalg.norm(low_embed[0::2] - low_embed[1::2], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "dist_ft = []\n",
    "low_embed = %time TSNE().fit_transform(all_embedding[n:])[:plot_n]\n",
    "for i, lang in enumerate([\"en\", \"ar\"]):\n",
    "  ax[1].scatter(low_embed[i::2, 0], low_embed[i::2, 1], marker=markers[i], color=colors[i], s=100, alpha=0.2, label=lang)\n",
    "  \n",
    "dist_ft = np.linalg.norm(low_embed[0::2] - low_embed[1::2], axis=1)\n",
    "  \n",
    "ax[0].set_title(\"mBERT\")\n",
    "ax[1].set_title(\"mBERT + TPP\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[0].legend(loc=\"upper left\")\n",
    "fig.tight_layout()\n",
    "plt.savefig(Path(\"../figures/en_ar_embeddings.pdf\").expanduser(), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_file = Path(\"../data/en_ar_tatoeba.json\").expanduser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "with data_file.open() as fp:\n",
    "    for i, line in enumerate(fp):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if i > 1000:\n",
    "            break\n",
    "        line = json.loads(line)\n",
    "        line_sents = line[\"unique_label_desc\"]\n",
    "        sentences.extend(line_sents)\n",
    "        labels.extend([i] * len(line_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.asarray(sentences)\n",
    "sentences[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lang_dists = pd.DataFrame(\n",
    "    {lang: sentences[:n][i::2] for i, lang in enumerate([\"en\", \"ar\"])}\n",
    ").assign(dist_mbert=dist_mbert, dist_ft=dist_ft, total_delta_dist=total_delta_dist)\n",
    "\n",
    "df_lang_dists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_idx = [1, 2, 37, 39]\n",
    "with pd.option_context(\"display.float_format\", \"{:.1f}\".format):\n",
    "    display(df_lang_dists.loc[paper_idx])\n",
    "    print(\n",
    "        df_lang_dists.loc[paper_idx]\n",
    "        .assign(ar=lambda x: x.ar.apply(lambda k: f\"RL{{ {k} }}\"))\n",
    "        .to_latex()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mldash_entity": {
   "created_at_millis": 1632169374200,
   "hash": "c11f1863c1aa741e46240628b892f60968680dee",
   "inferred_pdp_safe": false,
   "is_vfs_dir": false,
   "marked_pdp_safe": false,
   "notebook_id": "1440036539006795782",
   "owner": "smishra",
   "shared_to_everyone": false,
   "shared_to_ldap_groups": [],
   "shared_to_ldap_users": [],
   "size": 147288,
   "tags": [],
   "uuid": "1440048845467971589",
   "vfs_path": "/user/smishra/notebooks/NLPLib/Multilingual Conversations Plots.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
