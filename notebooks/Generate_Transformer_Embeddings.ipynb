{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "// Copyright 2020 Twitter, Inc.\n",
    "// SPDX-License-Identifier: Apache-2.0\n",
    "```\n",
    "\n",
    "# Generate Transformer Embeddings\n",
    "\n",
    "Take an existing BERT model (with or without TPP pre-training) generate embeddings from it on translation pairs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Fjip4YztNIh"
   },
   "source": [
    "## Setup libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79yDGUm8s66W"
   },
   "outputs": [],
   "source": [
    "%pip install transformers==3.5.1 datasets==1.1.2 torch==1.4.0 seqeval==1.2.2 gensim==3.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from transformers import AutoModel, BertTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path(\"../models/multi_tatoeba_2t_en_hi_ja_ar_equal_bce_model/\").expanduser()\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    str(model_dir), max_len=512, truncation=True, padding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(str(model_dir)).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PDP: Data is scrubbed. For more information visit go/pycx-pdp."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch = tokenizer.batch_encode_plus(\n",
    "    [\"This is a great world\", \"Obama went to Paris and Trump to London\"],\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PDP: Data is scrubbed. For more information visit go/pycx-pdp."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = model(**batch)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PDP: Data is scrubbed. For more information visit go/pycx-pdp."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[o.shape for o in output]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = Path(\"../data/en_ar_tatoeba.json\").expanduser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "with data_file.open() as fp:\n",
    "    for i, line in enumerate(fp):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if i > 1000:\n",
    "            break\n",
    "        line = json.loads(line)\n",
    "        line_sents = line[\"unique_label_desc\"]\n",
    "        sentences.extend(line_sents)\n",
    "        labels.extend([i] * len(line_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PDP: Data is scrubbed. For more information visit go/pycx-pdp."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for i in range(0, len(sentences), 20):\n",
    "    sents = sentences[i : i + 20]\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        sents, padding=True, max_length=512, return_tensors=\"pt\"\n",
    "    )\n",
    "    emb = model(**batch)[1].detach().numpy()\n",
    "    embeddings.append(emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = Path(\"../data/en_ar_embeddings.ft.npz\").expanduser()\n",
    "np.savez(embeddings_path, embeddings=embeddings, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"bert-base-multilingual-uncased\"\n",
    "model = AutoModel.from_pretrained(str(model_dir)).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for i in range(0, len(sentences), 20):\n",
    "    sents = sentences[i : i + 20]\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        sents, padding=True, max_length=512, return_tensors=\"pt\"\n",
    "    )\n",
    "    emb = model(**batch)[1].detach().numpy()\n",
    "    embeddings.append(emb)\n",
    "embeddings = np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = Path(\"../data/en_ar_embeddings.base.npz\").expanduser()\n",
    "np.savez(embeddings_path, embeddings=embeddings, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mldash_entity": {
   "created_at_millis": 1611851806925,
   "hash": "2676218fb9d5cdc5d804c3e48b64af8e5ef27c87",
   "inferred_pdp_safe": true,
   "is_vfs_dir": false,
   "marked_pdp_safe": false,
   "owner": "smishra",
   "shared_to_everyone": false,
   "shared_to_ldap_groups": [],
   "shared_to_ldap_users": [],
   "size": 3163,
   "tags": [],
   "uuid": "1354830792199991298",
   "vfs_path": "/user/smishra/notebooks/NLPLib/Generate Transformer Embeddings.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
